# Can you see this line of code

# -*- coding: utf-8 -*-
"""Copy of stock prize prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1edTyMelBp442Jt7JS83JDzjR2RADQJDg
"""

!pip install -U -q PyDrive
 
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials
 
# 1. Authenticate and create the PyDrive client.
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)
file_list = drive.ListFile({'q': "'1H3EtldNffWIY_Enq0ScdrJu2POtonu3L' in parents and trashed=false"}).GetList()
for file1 in file_list:
  print('title: %s, id: %s' % (file1['title'], file1['id']))

# type(file)
import csv

stock_downloaded = drive.CreateFile({'id': '1xLnw6J7wqB0dDqEJrXsCYJ2idfB632Cz'})
# stock_downloaded.GetContentFile('Stock_Price_MAX.csv')

#EXTRA POINTS FOR ADDING YOUR OWN CODE!
stock_downloaded = open('yahoo_stock.csv')
print(stock_downloaded)

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import pandas as pd
import io
import requests
import numpy as np
from sklearn import metrics
from keras.models import Sequential
from keras.layers.core import Dense, Activation, Dropout
from keras.callbacks import EarlyStopping
from keras.callbacks import ModelCheckpoint
import collections
from sklearn import preprocessing
import sklearn.feature_extraction.text as sk_text
from keras.callbacks import EarlyStopping, ModelCheckpoint
from keras import optimizers
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten, Embedding
from keras.layers import Conv2D, MaxPooling2D
from keras.layers import Conv1D, MaxPooling1D
import keras
from keras.preprocessing import sequence
from keras.layers import LSTM


# Encode text values to indexes(i.e. [1],[2],[3] for red,green,blue).
def encode_text_index(df, name):
    le = preprocessing.LabelEncoder()
    df[name] = le.fit_transform(df[name])
    return le.classes_
  
# Encode text values to dummy variables(i.e. [1,0,0],[0,1,0],[0,0,1] for red,green,blue)
def encode_text_dummy(df, name):
    dummies = pd.get_dummies(df[name])
    for x in dummies.columns:
        dummy_name = "{}-{}".format(name, x)
        df[dummy_name] = dummies[x]
    df.drop(name, axis=1, inplace=True)
    
# Encode a column to a range between normalized_low and normalized_high.
def encode_numeric_range(df, name, normalized_low=0, normalized_high=1,
                         data_low=None, data_high=None):
    if data_low is None:
        data_low = min(df[name])
        data_high = max(df[name])

    df[name] = ((df[name] - data_low) / (data_high - data_low)) \
               * (normalized_high - normalized_low) + normalized_low
 
# %matplotlib inline
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc


# Convert a Pandas dataframe to the x,y inputs that TensorFlow needs
def to_xy(df, target):
    result = []
    for x in df.columns:
        if x != target:
            result.append(x)
    # find out the type of the target column. 
    target_type = df[target].dtypes
    target_type = target_type[0] if isinstance(target_type, collections.Sequence) else target_type
    # Encode to int for classification, float otherwise. TensorFlow likes 32 bits.
    if target_type in (np.int64, np.int32):
        # Classification
        dummies = pd.get_dummies(df[target])
        return df[result].values.astype(np.float32), dummies.values.astype(np.float32)
    else:
        # Regression
        return df[result].values.astype(np.float32), df[target].values.astype(np.float32)
      
def class_connection(x):
  if(x=='normal.'):
    x=0
  else:
    x=1
  return x

import numpy as np

def to_sequences(seq_size, df ,data):
    x = []
    y = []

    for i in range(len(data)-SEQUENCE_SIZE-1):
        #print(i)
        window = df[i:(i+SEQUENCE_SIZE)]
        after_window = data[i+SEQUENCE_SIZE]
        #window = [[x] for x in window]
       #print("{} - {}".format(window,after_window))
        x.append(window)
        y.append(after_window)
        
    return np.array(x),np.array(y)
  
# Regression chart.
def chart_regression(pred,y,sort=True):
    t = pd.DataFrame({'pred' : pred, 'y' : y})
    if sort:
        t.sort_values(by=['y'],inplace=True)
    a = plt.plot(t['y'].tolist(),label='expected')
    b = plt.plot(t['pred'].tolist(),label='prediction')
    plt.ylabel('output')
    plt.legend()
    plt.show()
 
  # Nicely formatted time string
def hms_string(sec_elapsed):
    h = int(sec_elapsed / (60 * 60))
    m = int((sec_elapsed % (60 * 60)) / 60)
    s = sec_elapsed % 60
    return "{}:{:>02}:{:>05.2f}".format(h, m, s)

df_stock = pd.read_csv(stock_downloaded, encoding="utf-8")
df_stock[0:5]

df_stock.dtypes

df_stock=df_stock.drop(['Date', 'Adj Close'], axis=1)

df_stock = df_stock.dropna(how='any',axis=0)

encode_numeric_range(df_stock, 'Open')
encode_numeric_range(df_stock, 'High')
encode_numeric_range(df_stock, 'Low')
encode_numeric_range(df_stock, 'Volume')

#Preparing x and y
x,y = to_xy(df_stock, 'Close')

#splitting data into 70% training and 30% testing 
x_train, x_test, y_train, y_test = train_test_split (x, y, test_size=0.30, random_state=45)

x_train.shape,x_test.shape,y_train.shape,y_test.shape

#Activation: Relu 
#Optimization: Adam
#2 Hidden Layers: 50 and 25 neaurons respectively

#checkpointer
checkpointer = ModelCheckpoint(filepath="best_weights_adam_relu.hdf5", verbose=0, save_best_only=True) # save best model
for i in range(5):
    print(i)
    
    model_adam_relu = Sequential()
    
    # 50 neurons in 1st hidden layer
    model_adam_relu.add(Dense(50, input_dim=x_train.shape[1], activation='relu')) # Hidden 1 

    # 25 neurons in 2nd hidden layer
    model_adam_relu.add(Dense(25, activation='relu')) # Hidden 2
    
    model_adam_relu.add(Dense(1)) # Output

    # optimizer - Back Prop algo
    model_adam_relu.compile(loss='mean_squared_error', optimizer='adam')

    #  monitor
    monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=3, verbose=2, mode='auto')

    # epochs can be increased 500 or 1000
    model_adam_relu.fit(x_train,y_train, validation_data=(x_test,y_test),callbacks=[monitor,checkpointer],verbose=2,epochs=1000)

model_adam_relu.load_weights('best_weights_adam_relu.hdf5') # load weights from best model
